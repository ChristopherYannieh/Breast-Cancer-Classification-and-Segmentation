# -*- coding: utf-8 -*-
"""U-NetModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nQMU1odcymH3LQCvWh-WCly7Zv4zpWM7

# U-Net Model

## Getting the data
"""

# Mount drive
from google.colab import drive
drive.mount('/content/drive')

"""## Importing the Libraries"""

import os
import numpy as np
import pandas as pd
import re
import random
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from keras.metrics import MeanIoU

"""## Load Data"""

# Define a dictionary for image and mask storage
data = {'img' : [], 'mask' : []}

# Define data loader function
def LoadData(data, img_path, mask_path):
    IMG_SIZE = 256
    img_names = os.listdir(img_path)
    
    names = []
    mask_names = []
    unique_names = []
    
    for i in range(len(img_names )):
        unique_names.append(img_names[i].split(')')[0])
    
    unique_names = list(set(unique_names))
    
    for i in range(len(unique_names)):
        names.append(unique_names[i]+').png')
        mask_names.append(unique_names[i]+')_mask.png')
    
    img_address = img_path + '/'
    mask_address = mask_path + '/'
    
    for i in range(len(names)):
        img = plt.imread(img_address + names[i])
        mask = plt.imread(mask_address + mask_names[i])
        
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) 
        mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE))
        
        data['img'].append(img)
        data['mask'].append(mask)
        
    return data

benign_directory = '/content/drive/MyDrive/ZAKA AI/Artificial Intelligence Program/Capstone Project/breast-cancer-dataset/benign'
malignant_directory = '/content/drive/MyDrive/ZAKA AI/Artificial Intelligence Program/Capstone Project/breast-cancer-dataset/malignant'

data = LoadData(data, img_path = benign_directory, mask_path = benign_directory)
data = LoadData(data, img_path = malignant_directory, mask_path = malignant_directory)

len(data['img'])

len(data['mask'])

"""## Split Data"""

training_data = {'img' : [], 'mask' : []}
validation_data = {'img' : [], 'mask' : []}

# Sample 550 images for training, 82 images for validation & rest (15) for testing
for _ in range(550):
  i = random.randint(0, len(data['img']) - 1)

  img = data['img'][i]
  mask = data['mask'][i]

  training_data['img'].append(img)
  training_data['mask'].append(mask)

  del data['img'][i]
  del data['mask'][i]

len(training_data['img'])

for _ in range(82):
  i = random.randint(0, len(data['img']) - 1)

  img = data['img'][i]
  mask = data['mask'][i]

  validation_data['img'].append(img)
  validation_data['mask'].append(mask)

  del data['img'][i]
  del data['mask'][i]

len(validation_data['img'])

testing_data = data
len(testing_data['img'])

"""## Inspect Data"""

plt.figure(figsize = (6,6))
plt.subplot(1,2,1)
plt.imshow(training_data['img'][15])
plt.title('Ultrasound Image')
plt.subplot(1,2,2)
plt.imshow(training_data['mask'][15])
plt.title('Predicted Mask')
plt.show()

plt.figure(figsize = (6,6))
plt.subplot(1,2,1)
plt.imshow(training_data['img'][220])
plt.title('Ultrasound Image')
plt.subplot(1,2,2)
plt.imshow(training_data['mask'][220])
plt.title('Predicted Mask')
plt.show()

plt.figure(figsize = (6,6))
plt.subplot(1,2,1)
plt.imshow(training_data['img'][175])
plt.title('Ultrasound Image')
plt.subplot(1,2,2)
plt.imshow(training_data['mask'][175])
plt.title('Predicted Mask')
plt.show()

"""## Convolutional 2D Block"""

from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Input
from tensorflow.keras import Model

def Conv2dBlock(inputTensor, number_of_filters, kernel_size = 3, doBatchNorm = True):
    # First Convolution
    x = Conv2D(filters = number_of_filters, kernel_size = (kernel_size, kernel_size),
                              kernel_initializer = 'he_normal', padding = 'same') (inputTensor)
    
    if doBatchNorm:
        x = BatchNormalization()(x)
        
    x = Activation('relu')(x)
    
    # Second Convolution
    x = Conv2D(filters = number_of_filters, kernel_size = (kernel_size, kernel_size),
                              kernel_initializer = 'he_normal', padding = 'same') (x)
    if doBatchNorm:
        x = BatchNormalization()(x)
        
    x = Activation('relu')(x)
    
    return x

"""## Model Definition"""

def define_unet_model(inputImage, number_of_filters = 40, dropouts = 0.02, doBatchNorm = True):
    # Defining Encoder Path
    c1 = Conv2dBlock(inputImage, number_of_filters * 1, kernel_size = 3, doBatchNorm = doBatchNorm)
    p1 = MaxPooling2D((2,2))(c1)
    p1 = Dropout(dropouts)(p1)
    
    c2 = Conv2dBlock(p1, number_of_filters * 2, kernel_size = 3, doBatchNorm = doBatchNorm)
    p2 = MaxPooling2D((2,2))(c2)
    p2 = Dropout(dropouts)(p2)
    
    c3 = Conv2dBlock(p2, number_of_filters * 4, kernel_size = 3, doBatchNorm = doBatchNorm)
    p3 = MaxPooling2D((2,2))(c3)
    p3 = Dropout(dropouts)(p3)
    
    c4 = Conv2dBlock(p3, number_of_filters * 8, kernel_size = 3, doBatchNorm = doBatchNorm)
    p4 = MaxPooling2D((2,2))(c4)
    p4 = Dropout(dropouts)(p4)
    
    c5 = Conv2dBlock(p4, number_of_filters * 16, kernel_size = 3, doBatchNorm = doBatchNorm)
    
    # Defining Decoder Path
    u6 = Conv2DTranspose(number_of_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)
    u6 = concatenate([u6, c4])
    u6 = Dropout(dropouts)(u6)
    c6 = Conv2dBlock(u6, number_of_filters * 8, kernel_size = 3, doBatchNorm = doBatchNorm)
    
    u7 = Conv2DTranspose(number_of_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)
    u7 = concatenate([u7, c3])
    u7 = Dropout(dropouts)(u7)
    c7 = Conv2dBlock(u7, number_of_filters * 4, kernel_size = 3, doBatchNorm = doBatchNorm)
    
    u8 = Conv2DTranspose(number_of_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)
    u8 = concatenate([u8, c2])
    u8 = Dropout(dropouts)(u8)
    c8 = Conv2dBlock(u8, number_of_filters * 2, kernel_size = 3, doBatchNorm = doBatchNorm)
    
    u9 = Conv2DTranspose(number_of_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)
    u9 = concatenate([u9, c1])
    u9 = Dropout(dropouts)(u9)
    c9 = Conv2dBlock(u9, number_of_filters * 1, kernel_size = 3, doBatchNorm = doBatchNorm)
    
    output = Conv2D(1, (1, 1), activation = 'sigmoid')(c9)
    model = Model(inputs = [inputImage], outputs = [output])

    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy', MeanIoU(2)])
    return model

inputs = Input((256, 256, 3))
unet_model = define_unet_model(inputs, dropouts= 0.02)

"""## Training"""

history = unet_model.fit(np.array(training_data['img']), np.array(training_data['mask']), validation_data=(np.array(validation_data['img']), np.array(validation_data['mask'])), epochs = 100, verbose = 1)

plt.figure(figsize=(15,5))
plt.subplot(1,3,1)
plt.plot(history.history['loss'], label = 'Training Lloss')
plt.plot(history.history['val_loss'], label = 'Validation Loss')
plt.legend()
plt.grid(True)

plt.subplot(1,3,2)
plt.plot(history.history['accuracy'], label = 'Training Accuracy')
plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1,3,3)
plt.plot(history.history['mean_io_u'], label = 'Training IoU')
plt.plot(history.history['val_mean_io_u'], label = 'Validation IoU')
plt.legend()
plt.grid(True)

plt.show()

"""## Testing"""

from tensorflow.keras.models import load_model
model = load_model('U-NetModel.h5')

def predict(data, model):
    images = data['img']
    masks = data['mask']
    
    images = np.array(images)
    
    predictions = model.predict(images)
  
    return images, predictions, masks


def plot_prediction(image, prediction_mask, actual_mask):
    plt.figure(figsize=(9,9))
    
    plt.subplot(1,3,1)
    plt.imshow(image)
    plt.title('Actual Image')
    
    plt.subplot(1,3,2)
    plt.imshow(prediction_mask)
    plt.title('Predicted Mask')
    
    plt.subplot(1,3,3)
    plt.imshow(actual_mask)
    plt.title('Actual Mask')

images, predictions, masks = predict(data, unet_model)

for i in range(len(testing_data['img'])):
  plot_prediction(images[i], predictions[i][:,:,0], masks[i])

"""**Test with Normal Images**"""

normal_data = {'img': [], 'mask': []}
normal_directory = '/content/drive/MyDrive/ZAKA AI/Artificial Intelligence Program/Capstone Project/breast-cancer-dataset/normal'
normal_data = LoadData(normal_data, imgPath= normal_directory, maskPath= normal_directory, shape= 256)

len(normal_data['img'])

def predict_normal(normal_data, model):
    images = normal_data['img']
    masks = normal_data['mask']
    
    images = images[0:10]
    masks = masks[0:10]

    images = np.array(images)
    
    predictions = model.predict(images)
  
    return images, predictions, masks

imgs, pred, mks = predict_normal(normal_data, unet_model)

for i in range(len(normal_data['img'][0:10])):
  plot_prediction(imgs[i], pred[i][:,:,0], mks[i])

"""## Save Model"""

unet_model.save('U-NetModel.h5')